{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to check versions\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")  # Should be False\n",
    "print(f\"Device being used: {torch.device('cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from openunmix import simulate\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "%matplotlib inline\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_audio_unmix(audio_path):\n",
    "    \"\"\"\n",
    "    Separate audio using OpenUnmix\n",
    "    Returns estimates dictionary containing 'vocals' and 'accompaniment'\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    print(\"Loading audio...\")\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(0, keepdim=True)\n",
    "    \n",
    "    # Separate\n",
    "    print(\"Separating audio (this may take a few minutes)...\")\n",
    "    model = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq', pretrained=True)\n",
    "    estimates = model(waveform)\n",
    "    \n",
    "    return estimates, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"infer3.mp3\"\n",
    "\n",
    "# Separate audio\n",
    "estimates, sr = separate_audio_unmix(audio_path)\n",
    "\n",
    "# Convert to numpy for visualization\n",
    "vocals = estimates['vocals'].squeeze().numpy()\n",
    "accompaniment = estimates['accompaniment'].squeeze().numpy()\n",
    "\n",
    "# Plot and play each component\n",
    "components = {\n",
    "    \"Vocals\": vocals,\n",
    "    \"Instrumental\": accompaniment\n",
    "}\n",
    "\n",
    "for name, signal in components.items():\n",
    "    print(f\"\\nAnalyzing {name}...\")\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Waveform\n",
    "    plt.subplot(2, 1, 1)\n",
    "    librosa.display.waveshow(signal, sr=sr)\n",
    "    plt.title(f\"{name} - Waveform\")\n",
    "    \n",
    "    # Spectrogram\n",
    "    plt.subplot(2, 1, 2)\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(signal)), ref=np.max)\n",
    "    librosa.display.specshow(D, y_axis='log', x_axis='time', sr=sr)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f\"{name} - Spectrogram\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Playing {name}...\")\n",
    "    ipd.display(ipd.Audio(signal, rate=sr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unmixenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
