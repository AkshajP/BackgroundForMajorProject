{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First save the model from the model maker.\n",
    "- get audio file. \n",
    "- run librosa track tempo and convert to integer.\n",
    "- at this bpm generate segment timings [ 0.0, 0.5, 1.0,...] (for 120 bpm)\n",
    "- look for filters to apply here (band pass, etc) to filter noise from regular recordings\n",
    "- apply process_audio_and_save_pcp \n",
    "- load model weights into this file. (model.save_weights('model.keras'))\n",
    "- predict using pcps\n",
    "- display the mapped chords \n",
    "    - grouping the segment times for the same chord and displaying the corresponding one chord per segment group \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import librosa\n",
    "model = tf.keras.models.load_model('model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting audio file \n",
    "audio_filename = '0001_mix.mp3'\n",
    "y,sr = librosa.load(audio_filename)\n",
    "tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr )\n",
    "# frame is a segment that is sampled at a regular interval ex every 512 samples\n",
    "# beat is high level info on a regular pulse in music\n",
    "# since the tempo can be varying in the given audio \n",
    "# im trying with all beat frames. first frame being non zero doesnt cause issue \n",
    "beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "# beat times gives the length of audio segment equivalent to column 0 in the annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio file: 0001_infer.mp3\n",
      "Detected tempo: [92.28515625] BPM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Chord Progression:\n",
      "-----------------------------\n",
      "00:00.697 - 00:01.997: Fmaj\n",
      "00:01.997 - 00:02.647: Emin\n",
      "00:02.647 - 00:03.622: Gmin\n",
      "00:03.622 - 00:03.947: Amin\n",
      "00:03.947 - 00:04.272: Cmin\n",
      "00:04.272 - 00:04.598: A#maj\n",
      "00:04.598 - 00:04.934: Fmaj\n",
      "00:04.934 - 00:05.271: A#min\n",
      "00:05.271 - 00:05.596: Amin\n",
      "00:05.596 - 00:05.921: Emaj\n",
      "00:05.921 - 00:06.246: Fmaj\n",
      "00:06.246 - 00:06.571: Amin\n",
      "00:06.571 - 00:06.896: Amaj\n",
      "00:06.896 - 00:07.221: Fmaj\n",
      "00:07.221 - 00:07.546: Cmaj\n",
      "00:07.546 - 00:07.872: Dmin\n",
      "00:07.872 - 00:08.197: Cmaj\n",
      "00:08.197 - 00:08.522: Amin\n",
      "00:08.522 - 00:09.822: Fmaj\n",
      "00:09.822 - 00:10.147: Emin\n",
      "00:10.147 - 00:10.472: Dmin\n",
      "00:10.472 - 00:10.797: Gmaj\n",
      "00:10.797 - 00:11.122: Gmin\n",
      "00:11.122 - 00:11.459: Gmaj\n",
      "00:11.459 - 00:12.121: Amin\n",
      "00:12.121 - 00:12.446: Fmaj\n",
      "00:12.446 - 00:12.771: Gmaj\n",
      "00:12.771 - 00:13.096: Dmaj\n",
      "00:13.096 - 00:15.697: Fmaj\n",
      "00:15.697 - 00:16.022: Fmin\n",
      "00:16.022 - 00:17.647: Fmaj\n",
      "00:17.647 - 00:18.297: Emin\n",
      "00:18.297 - 00:19.284: Gmin\n",
      "00:19.284 - 00:19.621: Amin\n",
      "00:19.621 - 00:19.934: Cmaj\n",
      "00:19.934 - 00:20.248: A#maj\n",
      "00:20.248 - 00:20.584: Fmaj\n",
      "00:20.584 - 00:20.921: A#maj\n",
      "00:20.921 - 00:21.246: Amin\n",
      "00:21.246 - 00:21.571: Emaj\n",
      "00:21.571 - 00:21.896: Fmaj\n",
      "00:21.896 - 00:22.221: Amin\n",
      "00:22.221 - 00:22.547: Amaj\n",
      "00:22.547 - 00:22.872: Fmaj\n",
      "00:22.872 - 00:23.197: Cmaj\n",
      "00:23.197 - 00:23.522: Dmin\n",
      "00:23.522 - 00:23.847: Cmaj\n",
      "00:23.847 - 00:24.172: Amin\n",
      "00:24.172 - 00:25.472: Fmaj\n",
      "00:25.472 - 00:25.797: Emin\n",
      "00:25.797 - 00:26.122: Dmin\n",
      "00:26.122 - 00:26.448: Gmaj\n",
      "00:26.448 - 00:26.773: Gmin\n",
      "00:26.773 - 00:27.109: Gmaj\n",
      "00:27.109 - 00:27.771: Amin\n",
      "00:27.771 - 00:28.096: Fmaj\n",
      "00:28.096 - 00:28.421: Gmaj\n",
      "00:28.421 - 00:28.746: Dmaj\n",
      "00:28.746 - 00:31.347: Fmaj\n",
      "00:31.347 - 00:31.672: Fmin\n",
      "00:31.672 - 00:33.297: Fmaj\n",
      "00:33.297 - 00:33.948: Emin\n",
      "00:33.948 - 00:34.934: Gmin\n",
      "00:34.934 - 00:35.271: Amin\n",
      "00:35.271 - 00:35.585: Cmaj\n",
      "00:35.585 - 00:35.898: A#maj\n",
      "00:35.898 - 00:36.235: Fmaj\n",
      "00:36.235 - 00:36.571: A#min\n",
      "00:36.571 - 00:36.897: Amin\n",
      "00:36.897 - 00:37.222: Emaj\n",
      "00:37.222 - 00:37.547: Fmaj\n",
      "00:37.547 - 00:37.872: Amin\n",
      "00:37.872 - 00:38.197: Amaj\n",
      "00:38.197 - 00:38.522: Fmaj\n",
      "00:38.522 - 00:38.847: Cmaj\n",
      "00:38.847 - 00:39.172: Dmin\n",
      "00:39.172 - 00:39.497: Cmaj\n",
      "00:39.497 - 00:39.822: Amin\n",
      "00:39.822 - 00:41.123: Fmaj\n",
      "00:41.123 - 00:41.448: Emin\n",
      "00:41.448 - 00:41.773: Dmin\n",
      "00:41.773 - 00:42.098: Gmaj\n",
      "00:42.098 - 00:42.423: Gmin\n",
      "00:42.423 - 00:42.760: Gmaj\n",
      "00:42.760 - 00:43.421: Amin\n",
      "00:43.421 - 00:43.746: Fmaj\n",
      "00:43.746 - 00:44.071: Gmaj\n",
      "00:44.071 - 00:44.397: Dmaj\n",
      "00:44.397 - 00:46.997: Fmaj\n",
      "00:46.997 - 00:47.322: Fmin\n",
      "00:47.322 - 00:48.948: Fmaj\n",
      "00:48.948 - 00:49.598: Emin\n",
      "00:49.598 - 00:50.596: Gmin\n",
      "00:50.596 - 00:50.921: Amin\n",
      "00:50.921 - 00:51.246: Cmin\n",
      "00:51.246 - 00:51.572: A#maj\n",
      "00:51.572 - 00:51.897: Fmaj\n",
      "00:51.897 - 00:52.222: A#min\n",
      "00:52.222 - 00:52.547: Amin\n",
      "00:52.547 - 00:52.872: Emaj\n",
      "00:52.872 - 00:53.197: Fmaj\n",
      "00:53.197 - 00:53.522: Amin\n",
      "00:53.522 - 00:53.847: Amaj\n",
      "00:53.847 - 00:54.172: Fmaj\n",
      "00:54.172 - 00:54.497: Cmaj\n",
      "00:54.497 - 00:54.822: Dmin\n",
      "00:54.822 - 00:55.147: Cmaj\n",
      "00:55.147 - 00:55.472: Amin\n",
      "00:55.472 - 00:56.773: Fmaj\n",
      "00:56.773 - 00:57.098: Emin\n",
      "00:57.098 - 00:57.423: Dmin\n",
      "00:57.423 - 00:57.748: Gmaj\n",
      "00:57.748 - 00:58.073: Gmin\n",
      "00:58.073 - 00:58.410: Gmaj\n",
      "00:58.410 - 00:59.072: Amin\n",
      "00:59.072 - 00:59.397: Fmaj\n",
      "00:59.397 - 00:59.722: Gmaj\n",
      "00:59.722 - 01:00.047: Dmaj\n",
      "01:00.047 - 01:02.647: Fmaj\n",
      "01:02.647 - 01:02.973: Fmin\n",
      "01:02.973 - 01:04.621: Fmaj\n",
      "01:04.621 - 01:05.248: Emin\n",
      "01:05.248 - 01:06.247: Gmin\n",
      "01:06.247 - 01:06.572: Amin\n",
      "01:06.572 - 01:06.897: Cmaj\n",
      "01:06.897 - 01:07.222: A#maj\n",
      "01:07.222 - 01:07.547: Fmaj\n",
      "01:07.547 - 01:07.872: A#min\n",
      "01:07.872 - 01:08.197: Amin\n",
      "01:08.197 - 01:08.522: Emaj\n",
      "01:08.522 - 01:08.847: Fmaj\n",
      "01:08.847 - 01:09.172: Amin\n",
      "01:09.172 - 01:09.497: Amaj\n",
      "01:09.497 - 01:09.822: Fmaj\n",
      "01:09.822 - 01:10.147: Cmaj\n",
      "01:10.147 - 01:10.473: Dmin\n",
      "01:10.473 - 01:10.809: Cmaj\n",
      "01:10.809 - 01:11.146: Amin\n",
      "01:11.146 - 01:12.446: Fmaj\n",
      "01:12.446 - 01:12.760: Emin\n",
      "01:12.760 - 01:13.073: Dmin\n",
      "01:13.073 - 01:13.398: Gmaj\n",
      "01:13.398 - 01:13.723: Gmin\n",
      "01:13.723 - 01:14.060: Gmaj\n",
      "01:14.060 - 01:14.722: Amin\n",
      "01:14.722 - 01:15.047: Fmaj\n",
      "01:15.047 - 01:15.372: Gmaj\n",
      "01:15.372 - 01:15.697: Dmaj\n",
      "01:15.697 - 01:18.298: Fmaj\n",
      "01:18.298 - 01:18.623: Gmin\n",
      "01:18.623 - 01:18.948: Amin\n",
      "01:18.948 - 01:19.923: Fmaj\n",
      "01:19.923 - 01:20.248: Gmin\n",
      "01:20.248 - 01:20.573: D#maj\n",
      "01:20.573 - 01:20.898: Fmaj\n",
      "01:20.898 - 01:21.223: Amin\n",
      "01:21.223 - 01:21.885: Gmaj\n",
      "01:21.885 - 01:22.547: D#maj\n",
      "01:22.547 - 01:22.872: Dmin\n",
      "01:22.872 - 01:23.197: D#maj\n",
      "01:23.197 - 01:23.522: Gmin\n",
      "01:23.522 - 01:24.172: Fmaj\n",
      "01:24.172 - 01:24.822: A#maj\n",
      "01:24.822 - 01:25.148: Gmaj\n",
      "01:25.148 - 01:25.473: Dmin\n",
      "01:25.473 - 01:25.798: Gmaj\n",
      "01:25.798 - 01:26.123: Dmin\n",
      "01:26.123 - 01:26.448: A#maj\n",
      "01:26.448 - 01:26.773: D#maj\n",
      "01:26.773 - 01:27.446: A#maj\n",
      "01:27.446 - 01:27.760: Gmin\n",
      "01:27.760 - 01:28.073: D#maj\n",
      "01:28.073 - 01:28.398: A#maj\n",
      "01:28.398 - 01:28.723: A#min\n",
      "01:28.723 - 01:29.049: Gmin\n",
      "01:29.049 - 01:29.374: Amin\n",
      "01:29.374 - 01:30.697: Fmaj\n",
      "01:30.697 - 01:31.022: D#maj\n",
      "01:31.022 - 01:31.347: Fmaj\n",
      "01:31.347 - 01:31.672: Amin\n",
      "01:31.672 - 01:32.323: Gmaj\n",
      "01:32.323 - 01:32.973: D#maj\n",
      "01:32.973 - 01:33.298: Dmin\n",
      "01:33.298 - 01:33.948: Fmaj\n",
      "01:33.948 - 01:34.923: Gmin\n",
      "01:34.923 - 01:35.248: Fmaj\n",
      "01:35.248 - 01:35.573: A#maj\n",
      "01:35.573 - 01:35.898: Gmin\n",
      "01:35.898 - 01:36.549: A#maj\n",
      "01:36.549 - 01:37.199: Amin\n",
      "01:37.199 - 01:37.535: Fmaj\n",
      "01:37.535 - 01:37.872: Amin\n",
      "01:37.872 - 01:38.522: Gmaj\n",
      "01:38.522 - 01:38.847: D#maj\n",
      "01:38.847 - 01:39.172: Fmaj\n",
      "01:39.172 - 01:39.498: Gmin\n",
      "01:39.498 - 01:39.823: Amin\n",
      "01:39.823 - 01:41.123: Fmaj\n",
      "01:41.123 - 01:41.448: D#maj\n",
      "01:41.448 - 01:41.773: Fmaj\n",
      "01:41.773 - 01:42.098: Amin\n",
      "01:42.098 - 01:42.748: Gmin\n",
      "01:42.748 - 01:43.398: D#maj\n",
      "01:43.398 - 01:43.724: Gmaj\n",
      "01:43.724 - 01:44.049: Gmin\n",
      "01:44.049 - 01:44.374: Gmaj\n",
      "01:44.374 - 01:45.047: Fmaj\n",
      "01:45.047 - 01:45.372: A#maj\n",
      "01:45.372 - 01:45.697: Dmin\n",
      "01:45.697 - 01:46.998: Gmaj\n",
      "01:46.998 - 01:47.323: Fmin\n",
      "01:47.323 - 01:47.648: Amin\n",
      "01:47.648 - 01:48.298: D#maj\n",
      "01:48.298 - 01:48.623: Fmin\n",
      "01:48.623 - 01:48.948: Amin\n",
      "01:48.948 - 01:49.923: D#maj\n",
      "01:49.923 - 01:50.248: Gmaj\n",
      "01:50.248 - 01:50.573: Fmaj\n",
      "01:50.573 - 01:50.899: A#min\n",
      "01:50.899 - 01:51.549: Gmaj\n",
      "01:51.549 - 01:51.874: Dmaj\n",
      "01:51.874 - 01:52.199: Cmaj\n",
      "01:52.199 - 01:52.524: Amin\n",
      "01:52.524 - 01:53.522: Fmin\n",
      "01:53.522 - 01:54.498: D#maj\n",
      "01:54.498 - 01:54.823: Gmaj\n",
      "01:54.823 - 01:55.148: A#min\n",
      "01:55.148 - 01:55.473: Fmaj\n",
      "01:55.473 - 01:55.798: A#maj\n",
      "01:55.798 - 01:56.123: A#min\n",
      "01:56.123 - 01:56.448: Gmaj\n",
      "01:56.448 - 01:56.773: Dmin\n",
      "01:56.773 - 01:57.423: Fmaj\n",
      "01:57.423 - 01:57.748: Gmaj\n",
      "01:57.748 - 01:59.374: Gmin\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, InputLayer\n",
    "from model_maker import create_ffnn_model\n",
    "from pcp_module import pcp_vectorise_segment\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress all logs except errors\n",
    "tf.get_logger().setLevel('ERROR') \n",
    "\n",
    "def load_trained_model(model_path):\n",
    "    \"\"\"\n",
    "    Load the model with trained weights.\n",
    "    For inference, we only need the model architecture and weights, not the optimizer state.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First try to load as a complete model\n",
    "        model = load_model(model_path)\n",
    "    except:\n",
    "        # If that fails, create new model and load just the weights\n",
    "        model = create_ffnn_model()\n",
    "        # Load weights without optimizer state\n",
    "        model.load_weights(model_path, by_name=True, skip_mismatch=True)\n",
    "    \n",
    "    # Recompile the model for inference only (no training needed)\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "   \n",
    "\n",
    "def predict_chord(pcp_vector, model):\n",
    "    \"\"\"\n",
    "    Predict chord from PCP vector using the trained model\n",
    "    Returns the predicted chord label\n",
    "    \"\"\"\n",
    "    chord_list = ['Cmaj', 'Cmin', 'C#maj', 'C#min', 'Dmaj', 'Dmin', 'D#maj', 'D#min', \n",
    "                  'Emaj', 'Emin', 'Fmaj', 'Fmin', 'F#maj', 'F#min', 'Gmaj', 'Gmin', \n",
    "                  'G#maj', 'G#min', 'Amaj', 'Amin', 'A#maj', 'A#min', 'Bmaj', 'Bmin']\n",
    "    \n",
    "    # Reshape PCP vector for model input\n",
    "    pcp_vector = np.array(pcp_vector).reshape(1, -1)\n",
    "    \n",
    "    # Get model prediction\n",
    "    prediction = model.predict(pcp_vector, verbose=0)\n",
    "    chord_index = np.argmax(prediction)\n",
    "    \n",
    "    return chord_list[chord_index]\n",
    "\n",
    "def apply_audio_filters(audio_data, sr):\n",
    "    \"\"\"Apply audio filters to clean the signal\"\"\"\n",
    "    # Apply a bandpass filter (keeping frequencies between 50Hz and 2000Hz)\n",
    "    y_filtered = librosa.effects.preemphasis(audio_data)\n",
    "    \n",
    "    # Apply HPSS (Harmonic-Percussive Source Separation)\n",
    "    y_harmonic, _ = librosa.effects.hpss(y_filtered)\n",
    "    \n",
    "    return y_harmonic\n",
    "\n",
    "def group_consecutive_chords(times, chords):\n",
    "    \"\"\"Group consecutive identical chords and their time intervals\"\"\"\n",
    "    grouped_segments = []\n",
    "    \n",
    "    # Create pairs of (time, chord)\n",
    "    chord_segments = list(zip(times[:-1], times[1:], chords))\n",
    "    \n",
    "    # Group by chord\n",
    "    for chord, group in groupby(chord_segments, key=lambda x: x[2]):\n",
    "        group_list = list(group)\n",
    "        start_time = group_list[0][0]\n",
    "        end_time = group_list[-1][1]\n",
    "        grouped_segments.append((start_time, end_time, chord))\n",
    "    \n",
    "    return grouped_segments\n",
    "\n",
    "def infer_chords(audio_file, model_weights_path):\n",
    "    \"\"\"\n",
    "    Main inference function that processes audio and returns chord predictions\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    print(f\"Loading audio file: {audio_file}\")\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    \n",
    "    # Get tempo and beat frames\n",
    "    tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    print(f\"Detected tempo: {tempo} BPM\")\n",
    "    \n",
    "    # Convert beat frames to time\n",
    "    beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "    \n",
    "    # Generate half-beat times by interpolating between beats\n",
    "    half_beat_times = []\n",
    "    for i in range(len(beat_times) - 1):\n",
    "        start_time = beat_times[i]\n",
    "        end_time = beat_times[i + 1]\n",
    "        mid_time = start_time + (end_time - start_time) / 2\n",
    "        half_beat_times.extend([start_time, mid_time])\n",
    "    # Add the last beat time\n",
    "    half_beat_times.append(beat_times[-1])\n",
    "    \n",
    "    # Convert to numpy array for easier handling\n",
    "    times = np.array(half_beat_times)\n",
    "    \n",
    "    # Load trained model\n",
    "    model = load_trained_model(model_weights_path)\n",
    "    \n",
    "    # Process each segment\n",
    "    predictions = []\n",
    "    for i in range(len(times) - 1):\n",
    "        start_time = times[i]\n",
    "        end_time = times[i + 1]\n",
    "        \n",
    "        # Convert times to sample indices\n",
    "        start_idx = int(start_time * sr)\n",
    "        end_idx = int(end_time * sr)\n",
    "        \n",
    "        # Extract segment\n",
    "        segment = y[start_idx:end_idx]\n",
    "        \n",
    "        # Apply filters\n",
    "        filtered_segment = apply_audio_filters(segment, sr)\n",
    "        \n",
    "        # Get PCP vector\n",
    "        pcp_vector_str = pcp_vectorise_segment(filtered_segment, sr, f\"segment_{start_time}\")\n",
    "        pcp_vector = [float(x) for x in pcp_vector_str.strip('[]').split(',')]\n",
    "        \n",
    "        # Predict chord\n",
    "        chord = predict_chord(pcp_vector, model)\n",
    "        predictions.append(chord)\n",
    "    \n",
    "    # Group consecutive identical chords\n",
    "    grouped_segments = group_consecutive_chords(times, predictions)\n",
    "    \n",
    "    return grouped_segments\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Format time in seconds to MM:SS.mmm\"\"\"\n",
    "    minutes = int(seconds // 60)\n",
    "    seconds_remainder = seconds % 60\n",
    "    return f\"{minutes:02d}:{seconds_remainder:06.3f}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"0001_infer.mp3\"\n",
    "    model_weights_path = \"model.h5\"\n",
    "    \n",
    "    # Run inference\n",
    "    chord_segments = infer_chords(audio_file, model_weights_path)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nPredicted Chord Progression:\")\n",
    "    print(\"-----------------------------\")\n",
    "    for start_time, end_time, chord in chord_segments:\n",
    "        print(f\"{format_time(start_time)} - {format_time(end_time)}: {chord}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "'''NOISE REDUCTION CODE USING SPECTRAL SUBTRACTION - BUT A NOISE SAMPLE IS NEEDED FOR THIS TO WORK'''\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "def calculate_noise_profile(y, sr, frame_length=2048, hop_length=512, n_fft=2048):\n",
    "    \"\"\"\n",
    "    Calculate noise profile from an audio signal\n",
    "    \n",
    "    Parameters:\n",
    "        y (np.ndarray): Input audio signal\n",
    "        sr (int): Sampling rate\n",
    "        frame_length (int): Length of each frame\n",
    "        hop_length (int): Number of samples between frames\n",
    "        n_fft (int): Length of FFT\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Noise profile spectrum\n",
    "    \"\"\"\n",
    "    # Calculate spectrogram\n",
    "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length, win_length=frame_length)\n",
    "    mag_spec = np.abs(D)\n",
    "    \n",
    "    # Estimate noise profile (using median of first few frames)\n",
    "    noise_frames = 10\n",
    "    noise_profile = np.median(mag_spec[:, :noise_frames], axis=1)\n",
    "    \n",
    "    return noise_profile\n",
    "\n",
    "def spectral_subtraction(y, sr, noise_profile, frame_length=2048, hop_length=512, n_fft=2048, \n",
    "                        reduction_factor=1.0, smoothing=0.1):\n",
    "    \"\"\"\n",
    "    Perform spectral subtraction using the calculated noise profile\n",
    "    \n",
    "    Parameters:\n",
    "        y (np.ndarray): Input audio signal\n",
    "        sr (int): Sampling rate\n",
    "        noise_profile (np.ndarray): Pre-calculated noise profile\n",
    "        frame_length (int): Length of each frame\n",
    "        hop_length (int): Number of samples between frames\n",
    "        n_fft (int): Length of FFT\n",
    "        reduction_factor (float): Factor to control noise reduction strength\n",
    "        smoothing (float): Smoothing factor for noise reduction\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Noise-reduced audio signal\n",
    "    \"\"\"\n",
    "    # Calculate STFT\n",
    "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length, win_length=frame_length)\n",
    "    mag_spec = np.abs(D)\n",
    "    phase_spec = np.angle(D)\n",
    "    \n",
    "    # Reshape noise profile to match spectrogram\n",
    "    noise_profile = noise_profile.reshape(-1, 1)\n",
    "    \n",
    "    # Subtract noise profile from magnitude spectrogram\n",
    "    mag_spec_reduced = mag_spec - (reduction_factor * noise_profile)\n",
    "    \n",
    "    # Apply flooring to avoid negative values\n",
    "    mag_spec_reduced = np.maximum(mag_spec_reduced, smoothing * mag_spec)\n",
    "    \n",
    "    # Reconstruct signal\n",
    "    D_reduced = mag_spec_reduced * np.exp(1j * phase_spec)\n",
    "    y_reduced = librosa.istft(D_reduced, hop_length=hop_length, win_length=frame_length)\n",
    "    \n",
    "    return y_reduced\n",
    "\n",
    "def reduce_noise(audio_path, output_path, noise_start_time=0, noise_duration=1.0):\n",
    "    \"\"\"\n",
    "    Complete noise reduction pipeline\n",
    "    \n",
    "    Parameters:\n",
    "        audio_path (str): Path to input audio file\n",
    "        output_path (str): Path to save processed audio\n",
    "        noise_start_time (float): Start time (in seconds) of noise sample\n",
    "        noise_duration (float): Duration (in seconds) of noise sample\n",
    "    \"\"\"\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    \n",
    "    # Extract noise sample\n",
    "    noise_start = int(noise_start_time * sr)\n",
    "    noise_length = int(noise_duration * sr)\n",
    "    noise_sample = y[noise_start:noise_start + noise_length]\n",
    "    \n",
    "    # Calculate noise profile\n",
    "    noise_profile = calculate_noise_profile(noise_sample, sr)\n",
    "    \n",
    "    # Apply noise reduction\n",
    "    y_reduced = spectral_subtraction(y, sr, noise_profile)\n",
    "    \n",
    "    y_reduced = librosa.util.normalize(y_reduced)\n",
    "    librosa.output.write_wav(output_path, y_reduced, sr)\n",
    "    \n",
    "    return y_reduced, sr\n",
    "\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_file = \"noisy_audio.wav\"\n",
    "#     output_file = \"cleaned_audio.wav\"\n",
    "    \n",
    "#     # Process the audio with custom parameters\n",
    "#     reduced_audio, sr = reduce_noise(\n",
    "#         input_file,\n",
    "#         output_file,\n",
    "#         noise_start_time=0,  # Assume noise sample is at the beginning\n",
    "#         noise_duration=1.0   # Use 1 second of noise for profile\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
