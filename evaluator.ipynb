{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "from model_maker import load_data_from_folder  # For PCP data\n",
    "from robust_model_maker import load_data_from_folder as load_robust_data  # For robust features\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_pcp_data(folder):\n",
    "    \"\"\"\n",
    "    Load PCP data and ensure correct shape\n",
    "    Returns X with 12 features and y with 24 classes\n",
    "    \"\"\"\n",
    "    X, y = load_data_from_folder(folder)\n",
    "    # Since the original function returns y, X we need to swap them\n",
    "    return y, X  # Now X has 12 features, y has 24 classes\n",
    "\n",
    "def load_and_preprocess_robust_data(folder):\n",
    "    \"\"\"\n",
    "    Load robust feature data and ensure correct shape\n",
    "    Returns X with 45 features and y with 24 classes\n",
    "    \"\"\"\n",
    "    X, y = load_robust_data(folder)\n",
    "    return y,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_list = ['Cmaj', 'Cmin', 'C#maj', 'C#min', 'Dmaj', 'Dmin', 'D#maj', 'D#min', \n",
    "                 'Emaj', 'Emin', 'Fmaj', 'Fmin', 'F#maj', 'F#min', 'Gmaj', 'Gmin', \n",
    "                 'G#maj', 'G#min', 'Amaj', 'Amin', 'A#maj', 'A#min', 'Bmaj', 'Bmin']\n",
    "print(\"\\nLoading PCP test data...\")\n",
    "X_test_pcp, y_test = load_and_preprocess_pcp_data(\"extracted_pcp_annotations_12_bin\")\n",
    "print(f\"PCP data shapes - X: {X_test_pcp.shape}, y: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading Robust test data...\")\n",
    "X_test_robust, y_test_robust = load_and_preprocess_robust_data(\"extracted_robust_45_annotations\")\n",
    "print(f\"Robust data shapes - X: {X_test_robust.shape}, y: {y_test_robust.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading PCP model...\")\n",
    "pcp_model = load_model(\"pcpmodel_1000.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, chord_list):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics including AUC for model evaluation\n",
    "    \"\"\"\n",
    "    # Basic metrics (same as before)\n",
    "    accuracy = accuracy_score(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true.argmax(axis=1), \n",
    "        y_pred.argmax(axis=1), \n",
    "        average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Calculate ROC AUC scores\n",
    "    # Micro-average: Calculate metrics globally by considering each element of the label indicator matrix as a label\n",
    "    auc_micro = roc_auc_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    # Macro-average: Calculate metrics for each label, and find their unweighted mean\n",
    "    auc_macro = roc_auc_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    # Calculate AUC for each chord\n",
    "    per_chord_auc = {}\n",
    "    for i, chord in enumerate(chord_list):\n",
    "        per_chord_auc[chord] = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "    \n",
    "    # Per-class metrics (same as before)\n",
    "    class_precision, class_recall, class_f1, _ = precision_recall_fscore_support(\n",
    "        y_true.argmax(axis=1), \n",
    "        y_pred.argmax(axis=1)\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    \n",
    "    # Create per-chord performance dictionary\n",
    "    chord_performance = {}\n",
    "    for i, chord in enumerate(chord_list):\n",
    "        chord_performance[chord] = {\n",
    "            'precision': class_precision[i],\n",
    "            'recall': class_recall[i],\n",
    "            'f1': class_f1[i],\n",
    "            'auc': per_chord_auc[chord]\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc_micro': auc_micro,\n",
    "        'auc_macro': auc_macro,\n",
    "        'per_chord_auc': per_chord_auc,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'chord_performance': chord_performance\n",
    "    }\n",
    "\n",
    "def plot_auc_comparison(pcp_metrics, robust_metrics, chord_list, save_path):\n",
    "    \"\"\"\n",
    "    Plot AUC comparison for both models\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    x = np.arange(len(chord_list))\n",
    "    width = 0.35\n",
    "    \n",
    "    pcp_aucs = [pcp_metrics['per_chord_auc'][chord] for chord in chord_list]\n",
    "    robust_aucs = [robust_metrics['per_chord_auc'][chord] for chord in chord_list]\n",
    "    \n",
    "    plt.bar(x - width/2, pcp_aucs, width, label='PCP Model')\n",
    "    plt.bar(x + width/2, robust_aucs, width, label='Robust Model')\n",
    "    \n",
    "    plt.ylabel('AUC Score')\n",
    "    plt.title('Per-Chord AUC Comparison')\n",
    "    plt.xticks(x, chord_list, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def measure_inference_time(model, X_test, batch_sizes=[1, 8, 16, 32]):\n",
    "    \"\"\"\n",
    "    Measure average inference time across multiple batch sizes\n",
    "    \"\"\"\n",
    "    timing_results = {}\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        times = []\n",
    "        # Ensure we have enough samples\n",
    "        num_batches = min(100, len(X_test) // batch_size)\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            batch = X_test[i*batch_size:(i+1)*batch_size]\n",
    "            start_time = time.time()\n",
    "            model.predict(batch, verbose=0)\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        timing_results[batch_size] = {\n",
    "            'mean_time': np.mean(times),\n",
    "            'std_time': np.std(times),\n",
    "            'min_time': np.min(times),\n",
    "            'max_time': np.max(times)\n",
    "        }\n",
    "    \n",
    "    return timing_results\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, chord_list, title, save_path):\n",
    "    \"\"\"\n",
    "    Plot and save confusion matrix heatmap with chord labels\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=chord_list, yticklabels=chord_list)\n",
    "    plt.title(f'Confusion Matrix - {title}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_performance_comparison(pcp_metrics, robust_metrics, save_path):\n",
    "    \"\"\"\n",
    "    Plot performance comparison bar chart\n",
    "    \"\"\"\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    pcp_values = [pcp_metrics[m] for m in metrics]\n",
    "    robust_values = [robust_metrics[m] for m in metrics]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, pcp_values, width, label='PCP Model')\n",
    "    plt.bar(x + width/2, robust_values, width, label='Robust Model')\n",
    "    \n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating PCP Model...\")\n",
    "pcp_predictions = pcp_model.predict(X_test_pcp, verbose=0)\n",
    "pcp_metrics = calculate_metrics(y_test, pcp_predictions, chord_list)\n",
    "pcp_timing = measure_inference_time(pcp_model, X_test_pcp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading Robust model...\")\n",
    "robust_model = load_model(\"robust_extraction_1000files.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating Robust Model...\")\n",
    "robust_predictions = robust_model.predict(X_test_robust, verbose=0)\n",
    "robust_metrics = calculate_metrics(y_test_robust, robust_predictions, chord_list)\n",
    "robust_timing = measure_inference_time(robust_model, X_test_robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(pcp_metrics['confusion_matrix'], \n",
    "                         chord_list, 'PCP Model', 'evals/pcp_confusion.png')\n",
    "plot_confusion_matrix(robust_metrics['confusion_matrix'], \n",
    "                         chord_list, 'Robust Model', 'evals/robust_confusion.png')\n",
    "plot_performance_comparison(pcp_metrics, robust_metrics, 'evals/model_comparison.png')\n",
    "plot_auc_comparison(pcp_metrics, robust_metrics, chord_list, 'evals/auc_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Model Comparison Results ===\")\n",
    "    \n",
    "print(\"\\nBasic Metrics:\")\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc_micro', 'auc_macro']\n",
    "for metric in metrics:\n",
    "    print(f\"\\n{metric.title()}:\")\n",
    "    print(f\"PCP Model: {pcp_metrics[metric]:.4f}\")\n",
    "    print(f\"Robust Model: {robust_metrics[metric]:.4f}\")\n",
    "\n",
    "# Save results to CSV with AUC metrics\n",
    "results = {\n",
    "    'Model': ['PCP Model', 'Robust Model'],\n",
    "    'Accuracy': [pcp_metrics['accuracy'], robust_metrics['accuracy']],\n",
    "    'Precision': [pcp_metrics['precision'], robust_metrics['precision']],\n",
    "    'Recall': [pcp_metrics['recall'], robust_metrics['recall']],\n",
    "    'F1 Score': [pcp_metrics['f1'], robust_metrics['f1']],\n",
    "    'AUC (Micro)': [pcp_metrics['auc_micro'], robust_metrics['auc_micro']],\n",
    "    'AUC (Macro)': [pcp_metrics['auc_macro'], robust_metrics['auc_macro']]\n",
    "}\n",
    "\n",
    "# Add timing results\n",
    "for batch_size in pcp_timing.keys():\n",
    "    results[f'Inference Time (ms) - Batch {batch_size}'] = [\n",
    "        pcp_timing[batch_size]['mean_time']*1000,\n",
    "        robust_timing[batch_size]['mean_time']*1000\n",
    "    ]\n",
    "\n",
    "# Save to CSV\n",
    "pd.DataFrame(results).to_csv('evals/model_comparison_results.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save per-chord performance including AUC\n",
    "chord_results = []\n",
    "for chord in chord_list:\n",
    "    chord_results.append({\n",
    "        'Chord': chord,\n",
    "        'PCP_Precision': pcp_metrics['chord_performance'][chord]['precision'],\n",
    "        'PCP_Recall': pcp_metrics['chord_performance'][chord]['recall'],\n",
    "        'PCP_F1': pcp_metrics['chord_performance'][chord]['f1'],\n",
    "        'PCP_AUC': pcp_metrics['chord_performance'][chord]['auc'],\n",
    "        'Robust_Precision': robust_metrics['chord_performance'][chord]['precision'],\n",
    "        'Robust_Recall': robust_metrics['chord_performance'][chord]['recall'],\n",
    "        'Robust_F1': robust_metrics['chord_performance'][chord]['f1'],\n",
    "        'Robust_AUC': robust_metrics['chord_performance'][chord]['auc']\n",
    "    })\n",
    "\n",
    "pd.DataFrame(chord_results).to_csv('evals/per_chord_performance.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
