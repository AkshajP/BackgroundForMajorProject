{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 20:38:28.179172: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-24 20:38:28.179266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-24 20:38:28.180388: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-24 20:38:28.187618: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-24 20:38:28.933970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "from model_maker import load_data_from_folder  # For PCP data\n",
    "from robust_model_maker import load_data_from_folder as load_robust_data  # For robust features\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    'pcpmodel_70_30_split.h5': {'type': 'pcp', 'split': '70_30'},\n",
    "    'pcpmodel_80_20_split.h5': {'type': 'pcp', 'split': '80_20'},\n",
    "    'pcpmodel_90_10_split.h5': {'type': 'pcp', 'split': '90_10'},\n",
    "    'robust_model_70_30_split.h5': {'type': 'robust', 'split': '70_30'},\n",
    "    'robust_model_80_20_split.h5': {'type': 'robust', 'split': '80_20'},\n",
    "    'robust_model_90_10_split.h5': {'type': 'robust', 'split': '90_10'}\n",
    "}\n",
    "\n",
    "# Chord list for reference\n",
    "CHORD_LIST = ['Cmaj', 'Cmin', 'C#maj', 'C#min', 'Dmaj', 'Dmin', 'D#maj', 'D#min', \n",
    "              'Emaj', 'Emin', 'Fmaj', 'Fmin', 'F#maj', 'F#min', 'Gmaj', 'Gmin', \n",
    "              'G#maj', 'G#min', 'Amaj', 'Amin', 'A#maj', 'A#min', 'Bmaj', 'Bmin']\n",
    "\n",
    "# Create output directory for evaluation results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f'evaluation_results_{timestamp}'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(model_type, split):\n",
    "    \"\"\"Load appropriate test data based on model type and split\"\"\"\n",
    "    \n",
    "    if model_type == 'pcp':\n",
    "        folder = f\"extracted_pcp_annotations_12_bin\"\n",
    "        X, y = load_data_from_folder(folder)\n",
    "        return y, X  # Swap to match expected format\n",
    "    else:  # robust\n",
    "        folder = f\"extracted_robust_45_annotations\"\n",
    "        X, y = load_robust_data(folder)\n",
    "        return y,X\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculate comprehensive metrics for model evaluation\"\"\"\n",
    "    accuracy = accuracy_score(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true.argmax(axis=1), \n",
    "        y_pred.argmax(axis=1), \n",
    "        average='weighted'\n",
    "    )\n",
    "    \n",
    "    # ROC AUC scores\n",
    "    auc_micro = roc_auc_score(y_true, y_pred, average='micro')\n",
    "    auc_macro = roc_auc_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    # Per-chord AUC\n",
    "    per_chord_auc = {}\n",
    "    for i, chord in enumerate(CHORD_LIST):\n",
    "        per_chord_auc[chord] = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc_micro': auc_micro,\n",
    "        'auc_macro': auc_macro,\n",
    "        'per_chord_auc': per_chord_auc,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "def measure_inference_time(model, X_test, batch_sizes=[1, 8, 16, 32]):\n",
    "    \"\"\"Measure inference time across different batch sizes\"\"\"\n",
    "    timing_results = {}\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        times = []\n",
    "        num_batches = min(100, len(X_test) // batch_size)\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            batch = X_test[i*batch_size:(i+1)*batch_size]\n",
    "            start_time = time.time()\n",
    "            model.predict(batch, verbose=0)\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        timing_results[batch_size] = {\n",
    "            'mean_time': np.mean(times),\n",
    "            'std_time': np.std(times),\n",
    "            'min_time': np.min(times),\n",
    "            'max_time': np.max(times)\n",
    "        }\n",
    "    \n",
    "    return timing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, model_name, output_dir):\n",
    "    \"\"\"Plot and save confusion matrix heatmap\"\"\"\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=CHORD_LIST, yticklabels=CHORD_LIST)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/{model_name}_confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_auc_comparison(metrics_dict, output_dir):\n",
    "    \"\"\"Plot AUC comparison across all models\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    models = list(metrics_dict.keys())\n",
    "    x = np.arange(len(CHORD_LIST))\n",
    "    width = 0.8 / len(models)\n",
    "    \n",
    "    for i, (model_name, metrics) in enumerate(metrics_dict.items()):\n",
    "        aucs = [metrics['per_chord_auc'][chord] for chord in CHORD_LIST]\n",
    "        plt.bar(x + i*width - width*len(models)/2, aucs, width, label=model_name)\n",
    "    \n",
    "    plt.ylabel('AUC Score')\n",
    "    plt.title('Per-Chord AUC Comparison Across Models')\n",
    "    plt.xticks(x, CHORD_LIST, rotation=45, ha='right')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/auc_comparison.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing pcp data loading for split 70_30:\n",
      "X shape: (272279, 12)\n",
      "y shape: (272279, 24)\n",
      "\n",
      "Testing pcp data loading for split 80_20:\n",
      "X shape: (272279, 12)\n",
      "y shape: (272279, 24)\n",
      "\n",
      "Testing pcp data loading for split 90_10:\n",
      "X shape: (272279, 12)\n",
      "y shape: (272279, 24)\n",
      "\n",
      "Testing robust data loading for split 70_30:\n",
      "X shape: (286913, 46)\n",
      "y shape: (286913, 24)\n",
      "\n",
      "Testing robust data loading for split 80_20:\n",
      "X shape: (286913, 46)\n",
      "y shape: (286913, 24)\n",
      "\n",
      "Testing robust data loading for split 90_10:\n",
      "X shape: (286913, 46)\n",
      "y shape: (286913, 24)\n"
     ]
    }
   ],
   "source": [
    "# Debug cell - Run this first to verify data loading\n",
    "for model_type in ['pcp', 'robust']:\n",
    "    for split in ['70_30', '80_20', '90_10']:\n",
    "        print(f\"\\nTesting {model_type} data loading for split {split}:\")\n",
    "        X, y = load_and_preprocess_data(model_type, split)\n",
    "        if X is not None:\n",
    "            print(f\"X shape: {X.shape}\")\n",
    "            print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating pcpmodel_70_30_split.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 20:39:14.174427: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-24 20:39:14.200110: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-24 20:39:14.200179: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-24 20:39:14.203829: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-24 20:39:14.203943: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-24 20:39:14.203995: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-24 20:39:14.508670: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-24 20:39:14.508753: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-24 20:39:14.508763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-11-24 20:39:14.508815: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-24 20:39:14.508834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2255 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for pcpmodel_70_30_split.h5\n",
      "\n",
      "Evaluating pcpmodel_80_20_split.h5...\n",
      "Direct model loading failed for pcpmodel_80_20_split.h5, trying weights loading...\n",
      "Completed evaluation for pcpmodel_80_20_split.h5\n",
      "\n",
      "Evaluating pcpmodel_90_10_split.h5...\n",
      "Completed evaluation for pcpmodel_90_10_split.h5\n",
      "\n",
      "Evaluating robust_model_70_30_split.h5...\n",
      "Direct model loading failed for robust_model_70_30_split.h5, trying weights loading...\n",
      "Completed evaluation for robust_model_70_30_split.h5\n",
      "\n",
      "Evaluating robust_model_80_20_split.h5...\n",
      "Direct model loading failed for robust_model_80_20_split.h5, trying weights loading...\n",
      "Completed evaluation for robust_model_80_20_split.h5\n",
      "\n",
      "Evaluating robust_model_90_10_split.h5...\n",
      "Direct model loading failed for robust_model_90_10_split.h5, trying weights loading...\n",
      "Completed evaluation for robust_model_90_10_split.h5\n"
     ]
    }
   ],
   "source": [
    "# Main evaluation loop\n",
    "all_metrics = {}\n",
    "all_timing = {}\n",
    "\n",
    "# Helper function to create model architecture\n",
    "def create_model_architecture(model_type):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, InputLayer\n",
    "    \n",
    "    if model_type == 'pcp':\n",
    "        model = Sequential([\n",
    "            InputLayer(input_shape=(12,)),\n",
    "            Dense(24, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(48, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(48, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(24, activation='sigmoid')\n",
    "        ])\n",
    "    else:  # robust model\n",
    "        model = Sequential([\n",
    "        InputLayer(input_shape=(46,)),\n",
    "        Dense(64, activation='elu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(96, activation='elu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='elu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='elu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(24, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name, config in MODEL_CONFIGS.items():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # First try loading the model directly\n",
    "        model_path = f\"models/{model_name}\"\n",
    "        try:\n",
    "            model = load_model(model_path)\n",
    "        except:\n",
    "            # If direct loading fails, create model and load weights\n",
    "            print(f\"Direct model loading failed for {model_name}, trying weights loading...\")\n",
    "            model = create_model_architecture(config['type'])\n",
    "            model.load_weights(model_path)\n",
    "    \n",
    "        # Load appropriate test data\n",
    "        X_test, y_test = load_and_preprocess_data(config['type'], config['split'])\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(X_test, verbose=0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(y_test, predictions, model_name)\n",
    "        all_metrics[model_name] = metrics\n",
    "        \n",
    "        # Measure inference time\n",
    "        timing = measure_inference_time(model, X_test)\n",
    "        all_timing[model_name] = timing\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(metrics['confusion_matrix'], model_name, output_dir)\n",
    "        \n",
    "        print(f\"Completed evaluation for {model_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {model_name}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(metrics)\n",
    "#print(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete. Results saved in evaluation_results_20241124_203829/\n"
     ]
    }
   ],
   "source": [
    "# Prepare results DataFrame\n",
    "results = []\n",
    "for model_name, metrics in all_metrics.items():\n",
    "    result = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1 Score': metrics['f1'],\n",
    "        'AUC (Micro)': metrics['auc_micro'],\n",
    "        'AUC (Macro)': metrics['auc_macro']\n",
    "    }\n",
    "    \n",
    "    # Add timing results\n",
    "    for batch_size, timing in all_timing[model_name].items():\n",
    "        result[f'Inference Time (ms) - Batch {batch_size}'] = timing['mean_time'] * 1000\n",
    "    \n",
    "    results.append(result)\n",
    "\n",
    "# Save overall results\n",
    "pd.DataFrame(results).to_csv(f'{output_dir}/model_comparison_results.csv', index=False)\n",
    "\n",
    "# Save per-chord performance\n",
    "chord_results = []\n",
    "for model_name, metrics in all_metrics.items():\n",
    "    for chord in CHORD_LIST:\n",
    "        chord_results.append({\n",
    "            'Model': model_name,\n",
    "            'Chord': chord,\n",
    "            'AUC': metrics['per_chord_auc'][chord]\n",
    "        })\n",
    "\n",
    "pd.DataFrame(chord_results).to_csv(f'{output_dir}/per_chord_performance.csv', index=False)\n",
    "\n",
    "print(f\"\\nEvaluation complete. Results saved in {output_dir}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
